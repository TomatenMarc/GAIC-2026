
Operator: Your next question comes from the line of Colin Sebastian with Robert Baird.
Colin Alan Sebastian: Great. Thanks, and good afternoon. First off, related to the machine learning capabilities and more specifically how that's deployed into content filtering, I wonder if you can compare the ability of the machines to analyze content today versus six months or even a year ago? Meaning, is that ability improving at a rate where you have a higher degree of confidence in that reliability? And then secondly, wonder if you've been able to discern any impact to date on content publishers or apps that are utilizing Facebook for reach and engagement following the rollout of changes in access to APIs, login and other developer resources. Thank you.
Mark Elliot Zuckerberg: I could take the first one. So on AI, I think that there's a very big shift in how we're going to think about content moderation on the platform. And I mean, this goes back to the beginning of the service, right? So in 2004, when I was starting in my dorm room, for a number of reasons, it was just me, so we didn't have a lot of capacity to have thousands of people reviewing content. AI technology was not developed at the time. The only real logical way to run the service was to enable people to share what they wanted and then reactively, if people in the community saw something that was offensive or they thought was against the rules, they'd flag it for us. And we'd look at it and take things down that didn't belong. Now it is becoming increasingly possible, both because we can build the AI tools, but couple that with being able to hire thousands and thousands of people to do faster review of the content and look at something proactively. We're shifting over the next few years to a much more proactive model of moderation. Now, one of the things that I think is going to be interesting, and in some cases a little frustrating, is that AI tools lend themselves towards identifying certain content a lot more easily than others. So one area where I'm very proud that we're doing great work is around identifying terrorist content. And I mentioned this before that 99% of the ISIS and Al Qaeda related content that we take down, we're removing before any person flags it to us. And that's great, right? That's doing a good job. But if you look at areas like hate speech, which are just much more nuanced linguistically, it really depends on the local language, that's an area where I think it's going to take more years to be able to do something reasonably. So one of the pieces of criticism that I think we get that I think is fair is we're much better able to enforce our nudity policies, for example, than we are hate speech. And the reason for that is it's much easier to build an AI system that can detect a nipple than it is to determine what is linguistically hate speech. So this is something that I think we will make progress on and will get better on over time. These are not unsolvable problems, although it's worth calling out that our adversaries have all the same AI tools â€“ or some of them, I think. I'd like to think that we're a little bit ahead. But we'll have a lot of the same tools as the field develops. But the combination of building AI and hiring what is going to be tens of thousands of people to work on these problems, I think will see us make very meaningful progress going forward.
Sheryl Kara Sandberg: To your second question, when we think about what's happening with developers, we are doing an audit of large developers and doing some investigation. We don't break out marketer segments, but mobile app install ads, which is where the revenue would come from developers, is a relatively small part of our advertising revenue. And our mobile app install ads help apps of all kinds, not those running on our platform. So we think the investigatory work we're doing into APIs, into the use, is very important, and we don't expect it to have an impact on revenue.