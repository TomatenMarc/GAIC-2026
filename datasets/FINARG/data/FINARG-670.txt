
Operator: Your next question comes from the line of Anthony DiClemente from Evercore.
Anthony DiClemente: Thank you very much for taking my question. Just really one for Mark, which is, I don't think anyone has asked much about the security investments that Facebook is making. And when I talk to investors, people are curious whether it's one-off or recurring. And so as you think about 2019, the magnitude of the resources that the company plans to deploy to protect privacy, to protect security, it sounds like you're in or will be in, hopefully, a better position to ward off bad actors than you were prior to the 2016 election. But I just wonder. Do you look at this as an endless arms race, or is there some point in this investment where you might be able to get some better efficiencies on those investments, also relative to others in the industry who are making investments that don't seem quite as sizable as Facebook's? Thanks.
Mark Elliot Zuckerberg: This is a really important question. I do think that we are up against sophisticated adversaries who will continue to evolve. So there is a large element of this, which is an arms race. And when you're talking about security issues and some of the safety and content issues, these are not problems that we fix. They're problems that you manage over time and try to reduce and prevent issues from coming up, but there's no silver bullet where you do the thing and then you're done. That said, I do think that we were quite behind where we needed to be a couple of years ago. We started a roadmap, which we said was going to be about a three-year roadmap. I think that we have some confidence in that timeframe, which takes us through the end of about 2019, to get our systems to the level that we generally think that they should be at, where we're building AI systems that can flag content that might be problematic to a much larger security and review team that can manage the larger volume of stuff that our tools are flagging to them. We're judging our success by, go through all of the categories of harmful content and behavior, whether it's terrorism, or self-harm, or hate speech, or just any different kind of thing that you'd be worried about. We're judging our success by how proactive can we get, so what percent of the stuff that we're taking down are we identifying before other people identify it for us. We've started issuing transparency reports so we can be held publicly accountable on this. What we see internally is that generally every week and every quarter that goes by, we're getting better and better at this. But I anticipate that it will be about the end of next year when we feel like we're as dialed in as we would generally all like us to be. And even at that point, we're not going to be perfect because more than 2 billion people are communicating on the service. There are going to be things that our systems miss, no matter how well-tuned we are. But I think we're making progress. We've made a lot of progress in the last couple of years on content overall. Elections are a special case, an extremely important special case of the content and safety issues and security issues that we face. But across all of the different types of content issues of people trying to spread hate or incite violence. We are making progress, and I feel good about the progress that we're making. And I think we will continue investing more. But I do think that to some degree the last few years and next year are probably going to be the biggest growth in the investment in the security efforts that we'll see.